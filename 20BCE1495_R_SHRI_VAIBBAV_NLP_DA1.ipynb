{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name: R SHRI VAIBBAV<br>\n",
        "Reg No: 20BCE1495<br>\n",
        "Course Code: CSE4022 Natural Language Processing<br>\n",
        "Faculty: Dr. V. Maria Anu**"
      ],
      "metadata": {
        "id": "0oHY5kQe9_Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tUtilize Python NLTK (Natural Language Tool Kit) Platform and do the following. Install relevant Packages and Libraries "
      ],
      "metadata": {
        "id": "qmGJOxeyXqNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "fGQBGooyMA3l"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "from urllib import request\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n"
      ],
      "metadata": {
        "id": "irYaX9LELwoS"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = brown.words()\n",
        "data = ' '.join([str(elem) for elem in text])\n",
        "data[:1000]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "2FPIVZSuMnRJ",
        "outputId": "724e7abc-1a27-4d49-d8d6-8e0cfae88f05"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. . `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' . The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' . It recommended that Fulton legislators act `` to have th\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1-Explore Brown Corpus and find the size, tokens, categories\n",
        "print(\"Explore Brown Corpus and find the size, tokens, categories\")\n",
        "print(brown.categories())\n",
        "len(brown.categories())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XiOEPfA2PnDU",
        "outputId": "7de5fd01-b08e-49e5-982d-7cb4a9f28044"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explore Brown Corpus and find the size, tokens, categories\n",
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2-Find the size of word tokens\n",
        "print(\"Find the size of word tokens\")\n",
        "print(len(word_tokenize(data)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zhb9QAN-Xs6x",
        "outputId": "c2bebe59-ab8a-4aca-c947-07014ee10b01"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Find the size of word tokens\n",
            "1173755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.3-Find the size of the category “government” \n",
        "print(\"Find the size of the category “government” \") \n",
        "print(len(brown.words(categories='government')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H89R38vBa-_W",
        "outputId": "7389bad5-e534-4aaa-f282-8bc997569352"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Find the size of the category “government” \n",
            "70117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4-List the most frequent tokens \n",
        "from nltk.probability import FreqDist\n",
        "print(\"Most Frequent tokens:\")\n",
        "fdist1 = FreqDist(data)\n",
        "fdist1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bD8RMilMcgzc",
        "outputId": "3e933f2b-a547-4c33-ff55-b1320e172870"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequent tokens:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({' ': 1161191, 'e': 589980, 't': 423392, 'a': 371418, 'o': 357020, 'i': 333212, 'n': 332908, 's': 300431, 'r': 287337, 'h': 249219, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5-Count the number of sentences \n",
        "print(\"Number of sentences:\") \n",
        "number_of_sentences = sent_tokenize(data)\n",
        "print(len(number_of_sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jXr8tnAEbcts",
        "outputId": "2bb47f05-c682-4e03-f6e0-a9df319c2597"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences:\n",
            "53190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tExplore the corpora available in NLTK (any two)     <br>                               \n",
        "•\tRaw corpus <br>\n",
        "•\tPOS tagged  <br>\n",
        "•\tParsed  <br>\n",
        "•\tMultilingual aligned <br>\n",
        "•\tSpoken language <br>\n",
        "•\tSemantic tagged \n"
      ],
      "metadata": {
        "id": "sr2RLwGhdux6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw Corpus**"
      ],
      "metadata": {
        "id": "vFqKEswVmPgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accessing Text from the Web \n",
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "type(raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s6aJH4T0mUeR",
        "outputId": "f1e0c25d-a62e-42c5-9d75-9b73fdca5c31"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FAZZNfyRm2kQ",
        "outputId": "0b8a01d4-b422-4a04-bd5f-d5d85c16dd9e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1176812"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7WBuGnmrm29Z",
        "outputId": "9d1253be-9567-4f39-8e3a-2c64a1806865"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Strings- Text Processing at the Lowest Level\n",
        "\n",
        "string = \"Hi my name is vaibbav and i am a cse student in VIT chennai.\"\n",
        "#Accessing Individual Characters\n",
        "print(string[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4lYYCQm4oFXW",
        "outputId": "bfc87c0a-a35a-4617-b5dd-85f3c269e976"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accessing Substrings\n",
        "print(string[-8:-1])\n",
        "print( string[6:21])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LP6EuIZ5oeW2",
        "outputId": "d795db56-d3cf-41b5-b4ff-82533bafc9da"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chennai\n",
            "name is vaibbav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string split by whitespaces\n",
        "print(\"\\nConverted String:\")\n",
        "print(string.split())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RJTFYBy8pTNI",
        "outputId": "34d4e03d-9271-473e-a428-1405840e8e86"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted String:\n",
            "['Hi', 'my', 'name', 'is', 'vaibbav', 'and', 'i', 'am', 'a', 'cse', 'student', 'in', 'VIT', 'chennai.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string to upper case\n",
        "print(\"\\nConverted String:\")\n",
        "print(string.upper())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Oj4EsLpmo_yj",
        "outputId": "ca8963b7-e0a2-4480-fbb5-bb0cbcaa176e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted String:\n",
            "HI MY NAME IS VAIBBAV AND I AM A CSE STUDENT IN VIT CHENNAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#string to lower case\n",
        "print(\"\\nConverted String:\")\n",
        "print(string.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KlxTC9gTpIZd",
        "outputId": "ec0e5121-d6d0-4b33-ff6e-32e6fdf828ef"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted String:\n",
            "hi my name is vaibbav and i am a cse student in vit chennai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting encoded text from files\n",
        "path = nltk.data.find(\"/content/sample.txt\")\n",
        "f = open(path, encoding='latin2')\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    print(line.encode('unicode_escape'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sfgRi58HqPkU",
        "outputId": "f065f74d-1072-4b09-9de6-9e057eed2dc5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Hi my name is vaibbav and i am a cse student in VIT chennai.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsed Corpora**"
      ],
      "metadata": {
        "id": "8DZxnBvAh0Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Treebank corpora provide a syntactic parse for each sentence. The NLTK data package includes a 10% sample of the Penn Treebank (in treebank), as well as the Sinica Treebank (in sinica_treebank)"
      ],
      "metadata": {
        "id": "YDnuSxMNidLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "\n",
        "#Reading the Penn Treebank (Wall Street Journal sample):\n",
        "print(treebank.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lcG6s66wduWg",
        "outputId": "37214118-7235-4167-ac18-5ee180c1f39a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', 'wsj_0005.mrg', 'wsj_0006.mrg', 'wsj_0007.mrg', 'wsj_0008.mrg', 'wsj_0009.mrg', 'wsj_0010.mrg', 'wsj_0011.mrg', 'wsj_0012.mrg', 'wsj_0013.mrg', 'wsj_0014.mrg', 'wsj_0015.mrg', 'wsj_0016.mrg', 'wsj_0017.mrg', 'wsj_0018.mrg', 'wsj_0019.mrg', 'wsj_0020.mrg', 'wsj_0021.mrg', 'wsj_0022.mrg', 'wsj_0023.mrg', 'wsj_0024.mrg', 'wsj_0025.mrg', 'wsj_0026.mrg', 'wsj_0027.mrg', 'wsj_0028.mrg', 'wsj_0029.mrg', 'wsj_0030.mrg', 'wsj_0031.mrg', 'wsj_0032.mrg', 'wsj_0033.mrg', 'wsj_0034.mrg', 'wsj_0035.mrg', 'wsj_0036.mrg', 'wsj_0037.mrg', 'wsj_0038.mrg', 'wsj_0039.mrg', 'wsj_0040.mrg', 'wsj_0041.mrg', 'wsj_0042.mrg', 'wsj_0043.mrg', 'wsj_0044.mrg', 'wsj_0045.mrg', 'wsj_0046.mrg', 'wsj_0047.mrg', 'wsj_0048.mrg', 'wsj_0049.mrg', 'wsj_0050.mrg', 'wsj_0051.mrg', 'wsj_0052.mrg', 'wsj_0053.mrg', 'wsj_0054.mrg', 'wsj_0055.mrg', 'wsj_0056.mrg', 'wsj_0057.mrg', 'wsj_0058.mrg', 'wsj_0059.mrg', 'wsj_0060.mrg', 'wsj_0061.mrg', 'wsj_0062.mrg', 'wsj_0063.mrg', 'wsj_0064.mrg', 'wsj_0065.mrg', 'wsj_0066.mrg', 'wsj_0067.mrg', 'wsj_0068.mrg', 'wsj_0069.mrg', 'wsj_0070.mrg', 'wsj_0071.mrg', 'wsj_0072.mrg', 'wsj_0073.mrg', 'wsj_0074.mrg', 'wsj_0075.mrg', 'wsj_0076.mrg', 'wsj_0077.mrg', 'wsj_0078.mrg', 'wsj_0079.mrg', 'wsj_0080.mrg', 'wsj_0081.mrg', 'wsj_0082.mrg', 'wsj_0083.mrg', 'wsj_0084.mrg', 'wsj_0085.mrg', 'wsj_0086.mrg', 'wsj_0087.mrg', 'wsj_0088.mrg', 'wsj_0089.mrg', 'wsj_0090.mrg', 'wsj_0091.mrg', 'wsj_0092.mrg', 'wsj_0093.mrg', 'wsj_0094.mrg', 'wsj_0095.mrg', 'wsj_0096.mrg', 'wsj_0097.mrg', 'wsj_0098.mrg', 'wsj_0099.mrg', 'wsj_0100.mrg', 'wsj_0101.mrg', 'wsj_0102.mrg', 'wsj_0103.mrg', 'wsj_0104.mrg', 'wsj_0105.mrg', 'wsj_0106.mrg', 'wsj_0107.mrg', 'wsj_0108.mrg', 'wsj_0109.mrg', 'wsj_0110.mrg', 'wsj_0111.mrg', 'wsj_0112.mrg', 'wsj_0113.mrg', 'wsj_0114.mrg', 'wsj_0115.mrg', 'wsj_0116.mrg', 'wsj_0117.mrg', 'wsj_0118.mrg', 'wsj_0119.mrg', 'wsj_0120.mrg', 'wsj_0121.mrg', 'wsj_0122.mrg', 'wsj_0123.mrg', 'wsj_0124.mrg', 'wsj_0125.mrg', 'wsj_0126.mrg', 'wsj_0127.mrg', 'wsj_0128.mrg', 'wsj_0129.mrg', 'wsj_0130.mrg', 'wsj_0131.mrg', 'wsj_0132.mrg', 'wsj_0133.mrg', 'wsj_0134.mrg', 'wsj_0135.mrg', 'wsj_0136.mrg', 'wsj_0137.mrg', 'wsj_0138.mrg', 'wsj_0139.mrg', 'wsj_0140.mrg', 'wsj_0141.mrg', 'wsj_0142.mrg', 'wsj_0143.mrg', 'wsj_0144.mrg', 'wsj_0145.mrg', 'wsj_0146.mrg', 'wsj_0147.mrg', 'wsj_0148.mrg', 'wsj_0149.mrg', 'wsj_0150.mrg', 'wsj_0151.mrg', 'wsj_0152.mrg', 'wsj_0153.mrg', 'wsj_0154.mrg', 'wsj_0155.mrg', 'wsj_0156.mrg', 'wsj_0157.mrg', 'wsj_0158.mrg', 'wsj_0159.mrg', 'wsj_0160.mrg', 'wsj_0161.mrg', 'wsj_0162.mrg', 'wsj_0163.mrg', 'wsj_0164.mrg', 'wsj_0165.mrg', 'wsj_0166.mrg', 'wsj_0167.mrg', 'wsj_0168.mrg', 'wsj_0169.mrg', 'wsj_0170.mrg', 'wsj_0171.mrg', 'wsj_0172.mrg', 'wsj_0173.mrg', 'wsj_0174.mrg', 'wsj_0175.mrg', 'wsj_0176.mrg', 'wsj_0177.mrg', 'wsj_0178.mrg', 'wsj_0179.mrg', 'wsj_0180.mrg', 'wsj_0181.mrg', 'wsj_0182.mrg', 'wsj_0183.mrg', 'wsj_0184.mrg', 'wsj_0185.mrg', 'wsj_0186.mrg', 'wsj_0187.mrg', 'wsj_0188.mrg', 'wsj_0189.mrg', 'wsj_0190.mrg', 'wsj_0191.mrg', 'wsj_0192.mrg', 'wsj_0193.mrg', 'wsj_0194.mrg', 'wsj_0195.mrg', 'wsj_0196.mrg', 'wsj_0197.mrg', 'wsj_0198.mrg', 'wsj_0199.mrg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(treebank.words('wsj_0003.mrg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uEQHat6Wi9Nt",
        "outputId": "e77b4417-54b3-4c8d-c1ad-cd06fec27b91"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'form', 'of', 'asbestos', 'once', 'used', '*', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(treebank.tagged_words('wsj_0003.mrg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kEwocqc7jN2a",
        "outputId": "bed71fca-c75e-4052-dc77-3c5deb95790c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(treebank.parsed_sents('wsj_0003.mrg')[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "myDJpBGnjTQ0",
        "outputId": "c7f191b9-274a-42e9-fe9e-2fb1a64840f7"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (S-TPC-2\n",
            "    (NP-SBJ\n",
            "      (NP (DT The) (NN asbestos) (NN fiber))\n",
            "      (, ,)\n",
            "      (NP (NN crocidolite))\n",
            "      (, ,))\n",
            "    (VP\n",
            "      (VBZ is)\n",
            "      (ADJP-PRD (RB unusually) (JJ resilient))\n",
            "      (SBAR-TMP\n",
            "        (IN once)\n",
            "        (S\n",
            "          (NP-SBJ (PRP it))\n",
            "          (VP (VBZ enters) (NP (DT the) (NNS lungs)))))\n",
            "      (, ,)\n",
            "      (PP\n",
            "        (IN with)\n",
            "        (S-NOM\n",
            "          (NP-SBJ\n",
            "            (NP (RB even) (JJ brief) (NNS exposures))\n",
            "            (PP (TO to) (NP (PRP it))))\n",
            "          (VP\n",
            "            (VBG causing)\n",
            "            (NP\n",
            "              (NP (NNS symptoms))\n",
            "              (SBAR\n",
            "                (WHNP-1 (WDT that))\n",
            "                (S\n",
            "                  (NP-SBJ (-NONE- *T*-1))\n",
            "                  (VP\n",
            "                    (VBP show)\n",
            "                    (PRT (RP up))\n",
            "                    (ADVP-TMP (NP (NNS decades)) (JJ later)))))))))))\n",
            "  (, ,)\n",
            "  (NP-SBJ (NNS researchers))\n",
            "  (VP (VBD said) (SBAR (-NONE- 0) (S (-NONE- *T*-2))))\n",
            "  (. .))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a text corpus with a minimum of 200 words (unique content). Implement the \n",
        "following text processing "
      ],
      "metadata": {
        "id": "izSfq2qsrChw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a text corpus\n",
        "\n",
        "import os\n",
        "import nltk.data\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "corpusdir = '/content/Corpus.txt' # Directory of corpus.\n",
        "\n",
        "newcorpus = PlaintextCorpusReader(corpusdir, '.*')\n",
        "\n",
        "newdata=nltk.data.load(corpusdir)\n",
        "print(newdata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FmIHBGdfrCP4",
        "outputId": "7e65134c-21b9-4825-a93a-39fc25eb00cb"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While natural language processing isn’t a new science, the technology is rapidly advancing thanks to an increased interest in human-to-machine communications, plus an availability of big data, powerful computing and enhanced algorithms. \r\n",
            "As a human, you may speak and write in English, Spanish or Chinese.\r\n",
            "But a computer’s native language – known as machine code or machine language – is largely incomprehensible to most people. \r\n",
            "At your device’s lowest levels, communication occurs not with words but through millions of zeros and ones that produce logical actions. \r\n",
            "Indeed, programmers used punch cards to communicate with the first computers 70 years ago. \r\n",
            "This manual and arduous process was understood by a relatively small number of people. \r\n",
            "Now you can say, “Alexa, I like this song,” and a device playing music in your home will lower the volume and reply, “OK. Rating saved,” in a humanlike voice. Then it adapts its algorithm to play that song – and others like it – the next time you listen to that music station.\r\n",
            "Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. \r\n",
            "For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important. \r\n",
            "Today’s machines can analyze more language-based data than humans, without fatigue and in a consistent, unbiased way. \r\n",
            "Considering the staggering amount of unstructured data that’s generated every day, from medical records to social media, automation will be critical to fully analyze text and speech data efficiently. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word segmentation\n",
        "words = nltk.word_tokenize(newdata)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g_YhJ4m2xtou",
        "outputId": "e3d75923-38d1-4491-b82a-8eac65635487"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['While', 'natural', 'language', 'processing', 'isn', '’', 't', 'a', 'new', 'science', ',', 'the', 'technology', 'is', 'rapidly', 'advancing', 'thanks', 'to', 'an', 'increased', 'interest', 'in', 'human-to-machine', 'communications', ',', 'plus', 'an', 'availability', 'of', 'big', 'data', ',', 'powerful', 'computing', 'and', 'enhanced', 'algorithms', '.', 'As', 'a', 'human', ',', 'you', 'may', 'speak', 'and', 'write', 'in', 'English', ',', 'Spanish', 'or', 'Chinese', '.', 'But', 'a', 'computer', '’', 's', 'native', 'language', '–', 'known', 'as', 'machine', 'code', 'or', 'machine', 'language', '–', 'is', 'largely', 'incomprehensible', 'to', 'most', 'people', '.', 'At', 'your', 'device', '’', 's', 'lowest', 'levels', ',', 'communication', 'occurs', 'not', 'with', 'words', 'but', 'through', 'millions', 'of', 'zeros', 'and', 'ones', 'that', 'produce', 'logical', 'actions', '.', 'Indeed', ',', 'programmers', 'used', 'punch', 'cards', 'to', 'communicate', 'with', 'the', 'first', 'computers', '70', 'years', 'ago', '.', 'This', 'manual', 'and', 'arduous', 'process', 'was', 'understood', 'by', 'a', 'relatively', 'small', 'number', 'of', 'people', '.', 'Now', 'you', 'can', 'say', ',', '“', 'Alexa', ',', 'I', 'like', 'this', 'song', ',', '”', 'and', 'a', 'device', 'playing', 'music', 'in', 'your', 'home', 'will', 'lower', 'the', 'volume', 'and', 'reply', ',', '“', 'OK', '.', 'Rating', 'saved', ',', '”', 'in', 'a', 'humanlike', 'voice', '.', 'Then', 'it', 'adapts', 'its', 'algorithm', 'to', 'play', 'that', 'song', '–', 'and', 'others', 'like', 'it', '–', 'the', 'next', 'time', 'you', 'listen', 'to', 'that', 'music', 'station', '.', 'Natural', 'language', 'processing', 'helps', 'computers', 'communicate', 'with', 'humans', 'in', 'their', 'own', 'language', 'and', 'scales', 'other', 'language-related', 'tasks', '.', 'For', 'example', ',', 'NLP', 'makes', 'it', 'possible', 'for', 'computers', 'to', 'read', 'text', ',', 'hear', 'speech', ',', 'interpret', 'it', ',', 'measure', 'sentiment', 'and', 'determine', 'which', 'parts', 'are', 'important', '.', 'Today', '’', 's', 'machines', 'can', 'analyze', 'more', 'language-based', 'data', 'than', 'humans', ',', 'without', 'fatigue', 'and', 'in', 'a', 'consistent', ',', 'unbiased', 'way', '.', 'Considering', 'the', 'staggering', 'amount', 'of', 'unstructured', 'data', 'that', '’', 's', 'generated', 'every', 'day', ',', 'from', 'medical', 'records', 'to', 'social', 'media', ',', 'automation', 'will', 'be', 'critical', 'to', 'fully', 'analyze', 'text', 'and', 'speech', 'data', 'efficiently', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence segmentation\n",
        "sentences = nltk.sent_tokenize(newdata)\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1pYeiCKaymOL",
        "outputId": "f0b1d775-20cf-4bfc-a699-6c42a332e83f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While natural language processing isn’t a new science, the technology is rapidly advancing thanks to an increased interest in human-to-machine communications, plus an availability of big data, powerful computing and enhanced algorithms.\n",
            "\n",
            "As a human, you may speak and write in English, Spanish or Chinese.\n",
            "\n",
            "But a computer’s native language – known as machine code or machine language – is largely incomprehensible to most people.\n",
            "\n",
            "At your device’s lowest levels, communication occurs not with words but through millions of zeros and ones that produce logical actions.\n",
            "\n",
            "Indeed, programmers used punch cards to communicate with the first computers 70 years ago.\n",
            "\n",
            "This manual and arduous process was understood by a relatively small number of people.\n",
            "\n",
            "Now you can say, “Alexa, I like this song,” and a device playing music in your home will lower the volume and reply, “OK.\n",
            "\n",
            "Rating saved,” in a humanlike voice.\n",
            "\n",
            "Then it adapts its algorithm to play that song – and others like it – the next time you listen to that music station.\n",
            "\n",
            "Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks.\n",
            "\n",
            "For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important.\n",
            "\n",
            "Today’s machines can analyze more language-based data than humans, without fatigue and in a consistent, unbiased way.\n",
            "\n",
            "Considering the staggering amount of unstructured data that’s generated every day, from medical records to social media, automation will be critical to fully analyze text and speech data efficiently.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Lowercase\n",
        "print(newdata.lower())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FX7Lzb_uy92G",
        "outputId": "f87d4005-b9ab-41ac-d378-1509076b6c51"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "while natural language processing isn’t a new science, the technology is rapidly advancing thanks to an increased interest in human-to-machine communications, plus an availability of big data, powerful computing and enhanced algorithms. \r\n",
            "as a human, you may speak and write in english, spanish or chinese.\r\n",
            "but a computer’s native language – known as machine code or machine language – is largely incomprehensible to most people. \r\n",
            "at your device’s lowest levels, communication occurs not with words but through millions of zeros and ones that produce logical actions. \r\n",
            "indeed, programmers used punch cards to communicate with the first computers 70 years ago. \r\n",
            "this manual and arduous process was understood by a relatively small number of people. \r\n",
            "now you can say, “alexa, i like this song,” and a device playing music in your home will lower the volume and reply, “ok. rating saved,” in a humanlike voice. then it adapts its algorithm to play that song – and others like it – the next time you listen to that music station.\r\n",
            "natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. \r\n",
            "for example, nlp makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important. \r\n",
            "today’s machines can analyze more language-based data than humans, without fatigue and in a consistent, unbiased way. \r\n",
            "considering the staggering amount of unstructured data that’s generated every day, from medical records to social media, automation will be critical to fully analyze text and speech data efficiently. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop words removal\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(newdata)\n",
        "# converts the words in word_tokens to lower case and then checks whether \n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "  \n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "  \n",
        "print(word_tokens)\n",
        "print(\"\\nSentence after removing stop words:\\n\")\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oEm_IlFczpDJ",
        "outputId": "e5c0d237-2488-42b5-cfa1-6c03492ceea2"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['While', 'natural', 'language', 'processing', 'isn', '’', 't', 'a', 'new', 'science', ',', 'the', 'technology', 'is', 'rapidly', 'advancing', 'thanks', 'to', 'an', 'increased', 'interest', 'in', 'human-to-machine', 'communications', ',', 'plus', 'an', 'availability', 'of', 'big', 'data', ',', 'powerful', 'computing', 'and', 'enhanced', 'algorithms', '.', 'As', 'a', 'human', ',', 'you', 'may', 'speak', 'and', 'write', 'in', 'English', ',', 'Spanish', 'or', 'Chinese', '.', 'But', 'a', 'computer', '’', 's', 'native', 'language', '–', 'known', 'as', 'machine', 'code', 'or', 'machine', 'language', '–', 'is', 'largely', 'incomprehensible', 'to', 'most', 'people', '.', 'At', 'your', 'device', '’', 's', 'lowest', 'levels', ',', 'communication', 'occurs', 'not', 'with', 'words', 'but', 'through', 'millions', 'of', 'zeros', 'and', 'ones', 'that', 'produce', 'logical', 'actions', '.', 'Indeed', ',', 'programmers', 'used', 'punch', 'cards', 'to', 'communicate', 'with', 'the', 'first', 'computers', '70', 'years', 'ago', '.', 'This', 'manual', 'and', 'arduous', 'process', 'was', 'understood', 'by', 'a', 'relatively', 'small', 'number', 'of', 'people', '.', 'Now', 'you', 'can', 'say', ',', '“', 'Alexa', ',', 'I', 'like', 'this', 'song', ',', '”', 'and', 'a', 'device', 'playing', 'music', 'in', 'your', 'home', 'will', 'lower', 'the', 'volume', 'and', 'reply', ',', '“', 'OK', '.', 'Rating', 'saved', ',', '”', 'in', 'a', 'humanlike', 'voice', '.', 'Then', 'it', 'adapts', 'its', 'algorithm', 'to', 'play', 'that', 'song', '–', 'and', 'others', 'like', 'it', '–', 'the', 'next', 'time', 'you', 'listen', 'to', 'that', 'music', 'station', '.', 'Natural', 'language', 'processing', 'helps', 'computers', 'communicate', 'with', 'humans', 'in', 'their', 'own', 'language', 'and', 'scales', 'other', 'language-related', 'tasks', '.', 'For', 'example', ',', 'NLP', 'makes', 'it', 'possible', 'for', 'computers', 'to', 'read', 'text', ',', 'hear', 'speech', ',', 'interpret', 'it', ',', 'measure', 'sentiment', 'and', 'determine', 'which', 'parts', 'are', 'important', '.', 'Today', '’', 's', 'machines', 'can', 'analyze', 'more', 'language-based', 'data', 'than', 'humans', ',', 'without', 'fatigue', 'and', 'in', 'a', 'consistent', ',', 'unbiased', 'way', '.', 'Considering', 'the', 'staggering', 'amount', 'of', 'unstructured', 'data', 'that', '’', 's', 'generated', 'every', 'day', ',', 'from', 'medical', 'records', 'to', 'social', 'media', ',', 'automation', 'will', 'be', 'critical', 'to', 'fully', 'analyze', 'text', 'and', 'speech', 'data', 'efficiently', '.']\n",
            "\n",
            "Sentence after removing stop words:\n",
            "\n",
            "['While', 'natural', 'language', 'processing', '’', 'new', 'science', ',', 'technology', 'rapidly', 'advancing', 'thanks', 'increased', 'interest', 'human-to-machine', 'communications', ',', 'plus', 'availability', 'big', 'data', ',', 'powerful', 'computing', 'enhanced', 'algorithms', '.', 'As', 'human', ',', 'may', 'speak', 'write', 'English', ',', 'Spanish', 'Chinese', '.', 'But', 'computer', '’', 'native', 'language', '–', 'known', 'machine', 'code', 'machine', 'language', '–', 'largely', 'incomprehensible', 'people', '.', 'At', 'device', '’', 'lowest', 'levels', ',', 'communication', 'occurs', 'words', 'millions', 'zeros', 'ones', 'produce', 'logical', 'actions', '.', 'Indeed', ',', 'programmers', 'used', 'punch', 'cards', 'communicate', 'first', 'computers', '70', 'years', 'ago', '.', 'This', 'manual', 'arduous', 'process', 'understood', 'relatively', 'small', 'number', 'people', '.', 'Now', 'say', ',', '“', 'Alexa', ',', 'I', 'like', 'song', ',', '”', 'device', 'playing', 'music', 'home', 'lower', 'volume', 'reply', ',', '“', 'OK', '.', 'Rating', 'saved', ',', '”', 'humanlike', 'voice', '.', 'Then', 'adapts', 'algorithm', 'play', 'song', '–', 'others', 'like', '–', 'next', 'time', 'listen', 'music', 'station', '.', 'Natural', 'language', 'processing', 'helps', 'computers', 'communicate', 'humans', 'language', 'scales', 'language-related', 'tasks', '.', 'For', 'example', ',', 'NLP', 'makes', 'possible', 'computers', 'read', 'text', ',', 'hear', 'speech', ',', 'interpret', ',', 'measure', 'sentiment', 'determine', 'parts', 'important', '.', 'Today', '’', 'machines', 'analyze', 'language-based', 'data', 'humans', ',', 'without', 'fatigue', 'consistent', ',', 'unbiased', 'way', '.', 'Considering', 'staggering', 'amount', 'unstructured', 'data', '’', 'generated', 'every', 'day', ',', 'medical', 'records', 'social', 'media', ',', 'automation', 'critical', 'fully', 'analyze', 'text', 'speech', 'data', 'efficiently', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"process\", \"processing\", \"processed\", \"processers\", \"processes\"]\n",
        "  \n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OM92fiKR0WWu",
        "outputId": "54ca4b03-9da4-4645-a9e2-6343302c19a6"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "process  :  process\n",
            "processing  :  process\n",
            "processed  :  process\n",
            "processers  :  process\n",
            "processes  :  process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "print(\"communications :\", lemmatizer.lemmatize(\"communications\"))\n",
        "print(\"programmers :\", lemmatizer.lemmatize(\"programmers\"))\n",
        "  \n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"lowest :\", lemmatizer.lemmatize(\"lowest\", pos =\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ObBNNSTh2oDJ",
        "outputId": "19a2c042-a452-4047-89b7-c03a44cd25c6"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "communications : communication\n",
            "programmers : programmer\n",
            "lowest : low\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of speech tagger\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized = sent_tokenize(newdata)\n",
        "for i in tokenized:\n",
        "     \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        " \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        " \n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E5qjoDL54ErV",
        "outputId": "c9356a5d-e772-4972-e23a-c57332c67064"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('While', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('’', 'JJ'), ('new', 'JJ'), ('science', 'NN'), (',', ','), ('technology', 'NN'), ('rapidly', 'RB'), ('advancing', 'VBG'), ('thanks', 'NNS'), ('increased', 'VBD'), ('interest', 'NN'), ('human-to-machine', 'NN'), ('communications', 'NNS'), (',', ','), ('plus', 'CC'), ('availability', 'NN'), ('big', 'JJ'), ('data', 'NNS'), (',', ','), ('powerful', 'JJ'), ('computing', 'VBG'), ('enhanced', 'JJ'), ('algorithms', 'NN'), ('.', '.')]\n",
            "[('As', 'IN'), ('human', 'JJ'), (',', ','), ('may', 'MD'), ('speak', 'VB'), ('write', 'JJ'), ('English', 'NNP'), (',', ','), ('Spanish', 'JJ'), ('Chinese', 'NNP'), ('.', '.')]\n",
            "[('But', 'CC'), ('computer', 'NN'), ('’', 'VBP'), ('native', 'JJ'), ('language', 'NN'), ('–', 'VBD'), ('known', 'VBN'), ('machine', 'NN'), ('code', 'NN'), ('machine', 'NN'), ('language', 'NN'), ('–', 'NNP'), ('largely', 'RB'), ('incomprehensible', 'JJ'), ('people', 'NNS'), ('.', '.')]\n",
            "[('At', 'IN'), ('device', 'NN'), ('’', 'CD'), ('lowest', 'NN'), ('levels', 'NNS'), (',', ','), ('communication', 'NN'), ('occurs', 'NNS'), ('words', 'NNS'), ('millions', 'NNS'), ('zeros', 'VBP'), ('ones', 'NNS'), ('produce', 'VBP'), ('logical', 'JJ'), ('actions', 'NNS'), ('.', '.')]\n",
            "[('Indeed', 'RB'), (',', ','), ('programmers', 'NNS'), ('used', 'VBD'), ('punch', 'JJ'), ('cards', 'NNS'), ('communicate', 'VBP'), ('first', 'JJ'), ('computers', 'NNS'), ('70', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('.', '.')]\n",
            "[('This', 'DT'), ('manual', 'JJ'), ('arduous', 'JJ'), ('process', 'NN'), ('understood', 'VBD'), ('relatively', 'RB'), ('small', 'JJ'), ('number', 'NN'), ('people', 'NNS'), ('.', '.')]\n",
            "[('Now', 'RB'), ('say', 'VBP'), (',', ','), ('“', 'JJ'), ('Alexa', 'NNP'), (',', ','), ('I', 'PRP'), ('like', 'VBP'), ('song', 'RB'), (',', ','), ('”', 'NNP'), ('device', 'NN'), ('playing', 'VBG'), ('music', 'NN'), ('home', 'NN'), ('lower', 'RBR'), ('volume', 'NN'), ('reply', 'NN'), (',', ','), ('“', 'NNP'), ('OK', 'NNP'), ('.', '.')]\n",
            "[('Rating', 'VBG'), ('saved', 'VBN'), (',', ','), ('”', 'FW'), ('humanlike', 'JJ'), ('voice', 'NN'), ('.', '.')]\n",
            "[('Then', 'RB'), ('adapts', 'VBZ'), ('algorithm', 'JJ'), ('play', 'NN'), ('song', 'NN'), ('–', 'JJ'), ('others', 'NNS'), ('like', 'IN'), ('–', 'NNP'), ('next', 'JJ'), ('time', 'NN'), ('listen', 'JJ'), ('music', 'NN'), ('station', 'NN'), ('.', '.')]\n",
            "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('helps', 'VBZ'), ('computers', 'NNS'), ('communicate', 'VBP'), ('humans', 'NNS'), ('language', 'NN'), ('scales', 'VBZ'), ('language-related', 'JJ'), ('tasks', 'NNS'), ('.', '.')]\n",
            "[('For', 'IN'), ('example', 'NN'), (',', ','), ('NLP', 'NNP'), ('makes', 'VBZ'), ('possible', 'JJ'), ('computers', 'NNS'), ('read', 'VBP'), ('text', 'NN'), (',', ','), ('hear', 'JJ'), ('speech', 'NN'), (',', ','), ('interpret', 'NN'), (',', ','), ('measure', 'NN'), ('sentiment', 'NN'), ('determine', 'NN'), ('parts', 'NNS'), ('important', 'JJ'), ('.', '.')]\n",
            "[('Today', 'NN'), ('’', 'NNP'), ('machines', 'NNS'), ('analyze', 'VBP'), ('language-based', 'JJ'), ('data', 'NNS'), ('humans', 'NNS'), (',', ','), ('without', 'IN'), ('fatigue', 'JJ'), ('consistent', 'NN'), (',', ','), ('unbiased', 'JJ'), ('way', 'NN'), ('.', '.')]\n",
            "[('Considering', 'VBG'), ('staggering', 'VBG'), ('amount', 'NN'), ('unstructured', 'JJ'), ('data', 'NNS'), ('’', 'RB'), ('generated', 'VBD'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('medical', 'JJ'), ('records', 'NNS'), ('social', 'JJ'), ('media', 'NNS'), (',', ','), ('automation', 'NN'), ('critical', 'JJ'), ('fully', 'RB'), ('analyze', 'VBP'), ('text', 'JJ'), ('speech', 'NN'), ('data', 'NNS'), ('efficiently', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}